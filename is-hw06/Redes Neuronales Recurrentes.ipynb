{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aime', 0.8313446640968323), ('je', 0.8059909343719482), ('berlin', 0.7630714178085327), ('italy', 0.7508494853973389), ('france', 0.7272523641586304), ('rome', 0.7216489315032959), ('london', 0.720532238483429), ('north', 0.7159467935562134), ('northwest', 0.7115777730941772), ('england', 0.69968581199646)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josue/.virtualenvs/is/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "\n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "\n",
    "\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "xx = np.array([tokenizer(i) for i in X])\n",
    "\n",
    "wmodel = Word2Vec(xx, size=100, window=5, min_count=1, workers=4)\n",
    "print(wmodel.wv.similar_by_word('paris'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 50000\n",
      "Review 2000 of 50000\n",
      "Review 3000 of 50000\n",
      "Review 4000 of 50000\n",
      "Review 5000 of 50000\n",
      "Review 6000 of 50000\n",
      "Review 7000 of 50000\n",
      "Review 8000 of 50000\n",
      "Review 9000 of 50000\n",
      "Review 10000 of 50000\n",
      "Review 11000 of 50000\n",
      "Review 12000 of 50000\n",
      "Review 13000 of 50000\n",
      "Review 14000 of 50000\n",
      "Review 15000 of 50000\n",
      "Review 16000 of 50000\n",
      "Review 17000 of 50000\n",
      "Review 18000 of 50000\n",
      "Review 19000 of 50000\n",
      "Review 20000 of 50000\n",
      "Review 21000 of 50000\n",
      "Review 22000 of 50000\n",
      "Review 23000 of 50000\n",
      "Review 24000 of 50000\n",
      "Review 25000 of 50000\n",
      "Review 26000 of 50000\n",
      "Review 27000 of 50000\n",
      "Review 28000 of 50000\n",
      "Review 29000 of 50000\n",
      "Review 30000 of 50000\n",
      "Review 31000 of 50000\n",
      "Review 32000 of 50000\n",
      "Review 33000 of 50000\n",
      "Review 34000 of 50000\n",
      "Review 35000 of 50000\n",
      "Review 36000 of 50000\n",
      "Review 37000 of 50000\n",
      "Review 38000 of 50000\n",
      "Review 39000 of 50000\n",
      "Review 40000 of 50000\n",
      "Review 41000 of 50000\n",
      "Review 42000 of 50000\n",
      "Review 43000 of 50000\n",
      "Review 44000 of 50000\n",
      "Review 45000 of 50000\n",
      "Review 46000 of 50000\n",
      "Review 47000 of 50000\n",
      "Review 48000 of 50000\n",
      "Review 49000 of 50000\n"
     ]
    }
   ],
   "source": [
    "x_data = getAvgFeatureVecs(xx, wmodel, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.82640683e-02, -5.25184833e-02, -6.06967323e-02, -2.08924323e-01,\n",
       "        4.31311548e-01, -1.63994685e-01,  2.56766349e-01,  7.16731772e-02,\n",
       "        4.60990518e-01, -2.21212000e-01, -4.27607596e-02,  1.24408543e-01,\n",
       "       -2.05710679e-01, -1.71380360e-02, -2.85039395e-01,  3.13528985e-01,\n",
       "        1.11457884e-01, -1.70109257e-01, -2.36563936e-01,  6.48029000e-02,\n",
       "        3.33912492e-01, -2.70428747e-01,  3.13253224e-01,  1.16007179e-01,\n",
       "        2.54754163e-02, -2.60393977e-01, -5.81691191e-02, -3.45129460e-01,\n",
       "        5.06210029e-02,  1.23540200e-01,  4.76964377e-02, -2.03079894e-01,\n",
       "        1.12609025e-02,  4.31001514e-01,  1.30492270e-01, -5.52110612e-01,\n",
       "        4.94925678e-01,  5.28065115e-03,  1.59090891e-01, -2.24617690e-01,\n",
       "        1.75620932e-02, -2.16154262e-01,  3.00571620e-01,  1.37710005e-01,\n",
       "        1.60136953e-01, -6.96084276e-02, -2.71278709e-01,  2.36877024e-01,\n",
       "        7.05848634e-02,  1.63012311e-01,  6.36439621e-01, -1.00466698e-01,\n",
       "       -1.20906197e-01, -2.31111124e-01,  2.45272383e-01, -2.39525825e-01,\n",
       "        1.35919496e-01,  5.20208478e-02, -3.56688768e-01,  1.70131132e-01,\n",
       "       -3.84467244e-02,  1.85349554e-01,  7.85784543e-01,  5.10924160e-01,\n",
       "        6.26287937e-01,  2.30725527e-01, -3.27556908e-01,  1.85634628e-01,\n",
       "        1.88673869e-01, -8.07346078e-05, -1.70310408e-01, -4.91647184e-01,\n",
       "       -1.83324963e-01,  6.96706831e-01, -1.69247855e-02, -1.25203535e-01,\n",
       "        1.71078548e-01, -3.43585730e-01, -2.71844864e-01, -1.48603246e-01,\n",
       "       -2.40314356e-03, -3.48786749e-02,  3.43989313e-01,  8.60640481e-02,\n",
       "        4.15466636e-01,  2.00089678e-01, -1.38708875e-01,  1.10292710e-01,\n",
       "        1.32297650e-01,  6.76051602e-02,  8.02963972e-04,  9.11917746e-01,\n",
       "       -4.71845940e-02,  4.41813171e-01,  1.87300012e-01,  1.06520027e-01,\n",
       "        3.12747769e-02,  1.31793976e-01,  3.67625058e-02, -1.40155023e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = deepcopy(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "5        1\n",
       "6        1\n",
       "7        1\n",
       "8        1\n",
       "9        1\n",
       "10       0\n",
       "11       1\n",
       "12       0\n",
       "13       0\n",
       "14       1\n",
       "15       0\n",
       "16       0\n",
       "17       1\n",
       "18       0\n",
       "19       1\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       1\n",
       "26       0\n",
       "27       1\n",
       "28       0\n",
       "29       1\n",
       "        ..\n",
       "49970    1\n",
       "49971    0\n",
       "49972    1\n",
       "49973    1\n",
       "49974    0\n",
       "49975    1\n",
       "49976    0\n",
       "49977    1\n",
       "49978    0\n",
       "49979    1\n",
       "49980    0\n",
       "49981    0\n",
       "49982    0\n",
       "49983    1\n",
       "49984    0\n",
       "49985    0\n",
       "49986    0\n",
       "49987    0\n",
       "49988    0\n",
       "49989    0\n",
       "49990    0\n",
       "49991    0\n",
       "49992    0\n",
       "49993    1\n",
       "49994    1\n",
       "49995    0\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    1\n",
       "Name: sentiment, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(messages, labels, split_frac, random_seed=None):\n",
    "    \"\"\"\n",
    "    Zero Pad input messages\n",
    "    :param messages: Input list of encoded messages\n",
    "    :param labels: Input list of encoded labels\n",
    "    :param split_frac: Input float, training split percentage\n",
    "    :return: tuple of arrays train_x, val_x, test_x, train_y, val_y, test_y\n",
    "    \"\"\"\n",
    "    # make sure that number of messages and labels allign\n",
    "    assert len(messages) == len(labels)\n",
    "    # random shuffle data\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "    shuf_idx = np.random.permutation(len(messages))\n",
    "    messages_shuf = np.array(messages)[shuf_idx] \n",
    "    labels_shuf = np.array(labels)[shuf_idx]\n",
    "\n",
    "    #make splits\n",
    "    split_idx = int(len(messages_shuf)*split_frac)\n",
    "    train_x, val_x = messages_shuf[:split_idx], messages_shuf[split_idx:]\n",
    "    train_y, val_y = labels_shuf[:split_idx], labels_shuf[split_idx:]\n",
    "\n",
    "    test_idx = int(len(val_x)*0.5)\n",
    "    val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "    val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "    return train_x, val_x, test_x, train_y, val_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, test_x, train_y, val_y, test_y = train_val_test_split(dataset, y, split_frac=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Size\n",
      "Train set: \t\t(40000, 100) \n",
      "Validation set: \t(5000, 100) \n",
      "Test set: \t\t(5000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Set Size\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases, n_input, n_hidden):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    # 2-layer LSTM, each layer has n_hidden units.\n",
    "    # Average Accuracy= 95.20% at 50k iter\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "    # Average Accuracy= 90.60% 50k iter\n",
    "    # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above\n",
    "    # rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "# training_iters = 50000\n",
    "training_iters = 2\n",
    "display_step = 1000\n",
    "n_input = 40000\n",
    "vocab_size = 100\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, 40000])\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-332eee554b96>:16: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "pred = RNN(x, weights, biases, n_input, n_hidden)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs_, labels_, keep_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_embedding_layer(inputs_, vocab_size, embed_size):\n",
    "    \"\"\"\n",
    "    Create the embedding layer\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size):\n",
    "    \"\"\"\n",
    "    Create the LSTM layers\n",
    "    \"\"\"\n",
    "    lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\n",
    "    # Add dropout to the cell\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "    return initial_state, lstm_outputs, cell, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate):\n",
    "    \"\"\"\n",
    "    Create the Loss function and Optimizer\n",
    "    \"\"\"\n",
    "    predictions = tf.contrib.layers.fully_connected(lstm_outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    loss = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimzer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return predictions, loss, optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_accuracy(predictions, labels_):\n",
    "    \"\"\"\n",
    "    Create accuracy\n",
    "    \"\"\"\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \"\"\"\n",
    "    Batch Generator for Training\n",
    "    :param x: Input array of x data\n",
    "    :param y: Input array of y data\n",
    "    :param batch_size: Input int, size of batch\n",
    "    :return: generator that returns a tuple of our x batch and y batch\n",
    "    \"\"\"\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(train_x)//batch_size\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(initial_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y[:, None],\n",
    "                        keep_prob_: keep_prob,\n",
    "                        initial_state: state}\n",
    "                loss_, state, _,  batch_acc = sess.run([loss, final_state, optimizer, accuracy], feed_dict=feed)\n",
    "#                 train_acc.append(batch_acc)\n",
    "                \n",
    "#                 if (ii + 1) % n_batches == 0:\n",
    "                    \n",
    "#                     val_acc = []\n",
    "#                     val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "#                     for xx, yy in get_batches(val_x, val_y, batch_size):\n",
    "#                         feed = {inputs_: xx,\n",
    "#                                 labels_: yy[:, None],\n",
    "#                                 keep_prob_: 1,\n",
    "#                                 initial_state: val_state}\n",
    "#                         val_batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "#                         val_acc.append(val_batch_acc)\n",
    "                    \n",
    "#                     print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "#                           \"Batch: {}/{}...\".format(ii+1, n_batches),\n",
    "#                           \"Train Loss: {:.3f}...\".format(loss_),\n",
    "#                           \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "#                           \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)))\n",
    "    \n",
    "#         saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inputs and Hyperparameters\n",
    "lstm_sizes = [100, 50]\n",
    "# vocab_size = len(dataset) + 1 #add one for padding\n",
    "vocab_size = 250\n",
    "embed_size = 300\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[238,28] = -1 is not in [0, 250)\n\t [[node embedding_lookup (defined at <ipython-input-13-cb2fbbe0ea34>:7)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_inputs_0_4, embedding_lookup/axis)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-92-62ce9dd40b49>\", line 3, in <module>\n    learning_rate, keep_prob, train_x, val_x, train_y, val_y)\n  File \"<ipython-input-89-080f4794b678>\", line 5, in build_and_train_network\n    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n  File \"<ipython-input-13-cb2fbbe0ea34>\", line 7, in build_embedding_layer\n    embed = tf.nn.embedding_lookup(embedding, inputs_)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 313, in embedding_lookup\n    transform_fn=None)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2675, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3332, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[238,28] = -1 is not in [0, 250)\n\t [[node embedding_lookup (defined at <ipython-input-13-cb2fbbe0ea34>:7)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_inputs_0_4, embedding_lookup/axis)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[238,28] = -1 is not in [0, 250)\n\t [[{{node embedding_lookup}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_inputs_0_4, embedding_lookup/axis)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-62ce9dd40b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n\u001b[0;32m----> 3\u001b[0;31m                             learning_rate, keep_prob, train_x, val_x, train_y, val_y)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-080f4794b678>\u001b[0m in \u001b[0;36mbuild_and_train_network\u001b[0;34m(lstm_sizes, vocab_size, embed_size, epochs, batch_size, learning_rate, keep_prob, train_x, val_x, train_y, val_y)\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0mkeep_prob_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         initial_state: state}\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#                 train_acc.append(batch_acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[238,28] = -1 is not in [0, 250)\n\t [[node embedding_lookup (defined at <ipython-input-13-cb2fbbe0ea34>:7)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_inputs_0_4, embedding_lookup/axis)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-92-62ce9dd40b49>\", line 3, in <module>\n    learning_rate, keep_prob, train_x, val_x, train_y, val_y)\n  File \"<ipython-input-89-080f4794b678>\", line 5, in build_and_train_network\n    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n  File \"<ipython-input-13-cb2fbbe0ea34>\", line 7, in build_embedding_layer\n    embed = tf.nn.embedding_lookup(embedding, inputs_)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 313, in embedding_lookup\n    transform_fn=None)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2675, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3332, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/josue/.virtualenvs/is/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[238,28] = -1 is not in [0, 250)\n\t [[node embedding_lookup (defined at <ipython-input-13-cb2fbbe0ea34>:7)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_inputs_0_4, embedding_lookup/axis)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
