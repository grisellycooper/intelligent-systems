{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation for Sentiment Analysis\n",
    "\n",
    "The following algorithm uses a Backpropagation with a SGD optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142â€“150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively. For simplicity, I assembled the reviews in a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', ':)', ':)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset and preparing using word2vec (Exercise 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVecMethod(words, model, num_features):\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "\n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('shuffled_movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize each review. This will take approx 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df['review'])\n",
    "y = list(df['sentiment'])\n",
    "xx = []\n",
    "for i in X:\n",
    "    xx.append(tokenizer(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check some data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1974',\n",
       " 'teenager',\n",
       " 'martha',\n",
       " 'moxley',\n",
       " 'maggie',\n",
       " 'grace',\n",
       " 'moves',\n",
       " 'high',\n",
       " 'class',\n",
       " 'area',\n",
       " 'belle',\n",
       " 'greenwich',\n",
       " 'connecticut',\n",
       " 'mischief',\n",
       " 'night',\n",
       " 'eve',\n",
       " 'halloween',\n",
       " 'murdered',\n",
       " 'backyard',\n",
       " 'house',\n",
       " 'murder',\n",
       " 'remained',\n",
       " 'unsolved',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'years',\n",
       " 'later',\n",
       " 'writer',\n",
       " 'mark',\n",
       " 'fuhrman',\n",
       " 'christopher',\n",
       " 'meloni',\n",
       " 'former',\n",
       " 'la',\n",
       " 'detective',\n",
       " 'fallen',\n",
       " 'disgrace',\n",
       " 'perjury',\n",
       " 'j',\n",
       " 'simpson',\n",
       " 'trial',\n",
       " 'moved',\n",
       " 'idaho',\n",
       " 'decides',\n",
       " 'investigate',\n",
       " 'case',\n",
       " 'partner',\n",
       " 'stephen',\n",
       " 'weeks',\n",
       " 'andrew',\n",
       " 'mitchell',\n",
       " 'purpose',\n",
       " 'writing',\n",
       " 'book',\n",
       " 'locals',\n",
       " 'squirm',\n",
       " 'welcome',\n",
       " 'support',\n",
       " 'retired',\n",
       " 'detective',\n",
       " 'steve',\n",
       " 'carroll',\n",
       " 'robert',\n",
       " 'forster',\n",
       " 'charge',\n",
       " 'investigation',\n",
       " '70',\n",
       " 'discover',\n",
       " 'criminal',\n",
       " 'net',\n",
       " 'power',\n",
       " 'money',\n",
       " 'cover',\n",
       " 'murder',\n",
       " 'murder',\n",
       " 'greenwich',\n",
       " 'good',\n",
       " 'tv',\n",
       " 'movie',\n",
       " 'true',\n",
       " 'story',\n",
       " 'murder',\n",
       " 'fifteen',\n",
       " 'years',\n",
       " 'old',\n",
       " 'girl',\n",
       " 'committed',\n",
       " 'wealthy',\n",
       " 'teenager',\n",
       " 'whose',\n",
       " 'mother',\n",
       " 'kennedy',\n",
       " 'powerful',\n",
       " 'rich',\n",
       " 'family',\n",
       " 'used',\n",
       " 'influence',\n",
       " 'cover',\n",
       " 'murder',\n",
       " 'twenty',\n",
       " 'years',\n",
       " 'however',\n",
       " 'snoopy',\n",
       " 'detective',\n",
       " 'convicted',\n",
       " 'perjurer',\n",
       " 'disgrace',\n",
       " 'able',\n",
       " 'disclose',\n",
       " 'hideous',\n",
       " 'crime',\n",
       " 'committed',\n",
       " 'screenplay',\n",
       " 'shows',\n",
       " 'investigation',\n",
       " 'mark',\n",
       " 'last',\n",
       " 'days',\n",
       " 'martha',\n",
       " 'parallel',\n",
       " 'lack',\n",
       " 'emotion',\n",
       " 'dramatization',\n",
       " 'vote',\n",
       " 'seven',\n",
       " 'title',\n",
       " 'brazil',\n",
       " 'available']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the word2vec model and train with all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(xx, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some words and its distance to verify the model was trained correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josue/.virtualenvs/is/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('aime', 0.8245323300361633),\n",
       " ('je', 0.7812187671661377),\n",
       " ('berlin', 0.7514671683311462),\n",
       " ('italy', 0.746889591217041),\n",
       " ('london', 0.7405767440795898),\n",
       " ('france', 0.7285119295120239),\n",
       " ('north', 0.6963350772857666),\n",
       " ('england', 0.6910803318023682),\n",
       " ('2054', 0.6908023953437805),\n",
       " ('san', 0.686427116394043)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.7755012512207031),\n",
       " ('puppy', 0.7531970739364624),\n",
       " ('chicken', 0.6783939599990845),\n",
       " ('rat', 0.6774106621742249),\n",
       " ('freak', 0.6763298511505127),\n",
       " ('bugs', 0.6654431223869324),\n",
       " ('bite', 0.6646356582641602),\n",
       " ('bird', 0.6585801839828491),\n",
       " ('monkey', 0.6538642048835754),\n",
       " ('pet', 0.6523016691207886)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data with word2vec model. This will take 15 or 20 minutes, depends on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josue/.virtualenvs/is/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 50000\n",
      "Review 2000 of 50000\n",
      "Review 3000 of 50000\n",
      "Review 4000 of 50000\n",
      "Review 5000 of 50000\n",
      "Review 6000 of 50000\n",
      "Review 7000 of 50000\n",
      "Review 8000 of 50000\n",
      "Review 9000 of 50000\n",
      "Review 10000 of 50000\n",
      "Review 11000 of 50000\n",
      "Review 12000 of 50000\n",
      "Review 13000 of 50000\n",
      "Review 14000 of 50000\n",
      "Review 15000 of 50000\n",
      "Review 16000 of 50000\n",
      "Review 17000 of 50000\n",
      "Review 18000 of 50000\n",
      "Review 19000 of 50000\n",
      "Review 20000 of 50000\n",
      "Review 21000 of 50000\n",
      "Review 22000 of 50000\n",
      "Review 23000 of 50000\n",
      "Review 24000 of 50000\n",
      "Review 25000 of 50000\n",
      "Review 26000 of 50000\n",
      "Review 27000 of 50000\n",
      "Review 28000 of 50000\n",
      "Review 29000 of 50000\n",
      "Review 30000 of 50000\n",
      "Review 31000 of 50000\n",
      "Review 32000 of 50000\n",
      "Review 33000 of 50000\n",
      "Review 34000 of 50000\n",
      "Review 35000 of 50000\n",
      "Review 36000 of 50000\n",
      "Review 37000 of 50000\n",
      "Review 38000 of 50000\n",
      "Review 39000 of 50000\n",
      "Review 40000 of 50000\n",
      "Review 41000 of 50000\n",
      "Review 42000 of 50000\n",
      "Review 43000 of 50000\n",
      "Review 44000 of 50000\n",
      "Review 45000 of 50000\n",
      "Review 46000 of 50000\n",
      "Review 47000 of 50000\n",
      "Review 48000 of 50000\n",
      "Review 49000 of 50000\n"
     ]
    }
   ],
   "source": [
    "x_data = getAvgFeatureVecs(xx, model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.60379612e-01, -2.64186233e-01, -2.53893062e-03,  3.41735072e-02,\n",
       "       -2.85584480e-01,  8.62184241e-02,  8.96588862e-02, -2.35976726e-02,\n",
       "        6.79201409e-02,  1.34501606e-01, -1.96452677e-01, -1.52094781e-01,\n",
       "        4.47102606e-01, -5.54411672e-04, -5.46162605e-01,  3.55192304e-01,\n",
       "        3.39316428e-02, -8.76795053e-02,  4.94601399e-01,  5.14558852e-01,\n",
       "       -1.66057527e-01, -5.62029123e-01, -5.45754880e-02,  7.16634467e-02,\n",
       "       -1.84396118e-01, -2.16080338e-01,  9.07994956e-02,  2.66660064e-01,\n",
       "       -5.74254274e-01,  2.00708911e-01,  2.09721312e-01, -2.03400284e-01,\n",
       "       -1.79200754e-01,  8.54769349e-02,  3.16235870e-01, -2.53821462e-01,\n",
       "        3.49349678e-02, -1.05141506e-01, -9.36906114e-02, -3.21814865e-01,\n",
       "        4.50330526e-01, -5.60665056e-02,  3.21082503e-01,  2.02802762e-01,\n",
       "       -3.37554291e-02,  2.02241272e-01, -6.31508291e-01, -4.84763294e-01,\n",
       "        3.45244855e-02,  1.22525327e-01, -1.11612365e-01, -3.12163770e-01,\n",
       "        1.27353191e-01, -1.76064670e-04, -1.13658167e-01,  2.57042974e-01,\n",
       "        4.62993562e-01,  3.83194685e-01, -2.61957824e-01,  1.26334250e-01,\n",
       "       -4.74081844e-01,  7.00909019e-01,  1.12678476e-01, -4.35850620e-01,\n",
       "        9.02524665e-02, -7.28816986e-02,  1.81661904e-01, -1.73506498e-01,\n",
       "       -6.48054838e-01, -2.48345315e-01,  3.74518692e-01, -4.49654087e-03,\n",
       "       -2.72844195e-01,  3.71400900e-02, -2.93682385e-02, -1.29074752e-01,\n",
       "        1.86603352e-01, -3.04218382e-01,  8.08791742e-02, -1.58254370e-01,\n",
       "       -3.64396095e-01, -5.70047162e-02,  2.56009907e-01, -1.38252541e-01,\n",
       "       -9.19358581e-02,  1.06921643e-01,  1.84788227e-01,  2.21527934e-01,\n",
       "       -7.81990662e-02,  2.22278833e-01,  2.84666777e-01,  3.01368952e-01,\n",
       "       -5.76249957e-02, -5.34990206e-02, -3.69093508e-01, -2.57101178e-01,\n",
       "       -4.03250247e-01, -1.65904492e-01,  6.02651089e-02,  4.11669940e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [list(i) for i in x_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = deepcopy(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, yy in zip(dataset, y):\n",
    "    x.append(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a backpropagation algorithm using SGD (Exercise 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is implemented in the file `backpropagtion.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backpropagation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_iterations = 10\n",
    "hidden_layers = 6\n",
    "num_folds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Backpropagation(\n",
    "        learning_rate, num_iterations, hidden_layers, num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data using cross validation. The accyracy approximately should be higher than 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 1\n",
      "iter 2\n",
      "iter 3\n",
      "iter 4\n",
      "iter 5\n",
      "iter 6\n",
      "iter 7\n",
      "iter 8\n",
      "iter 9\n",
      "iter 0\n",
      "iter 1\n",
      "iter 2\n",
      "iter 3\n",
      "iter 4\n",
      "iter 5\n",
      "iter 6\n",
      "iter 7\n",
      "iter 8\n",
      "iter 9\n",
      "accuracy 86.11999999999999\n"
     ]
    }
   ],
   "source": [
    "model.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'I loved this movie'\n",
    "example = tokenizer(example)\n",
    "\n",
    "\n",
    "example = featureVecMethod(example, model, 100)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
